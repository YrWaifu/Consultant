"""
–ü–∞—Ä—Å–µ—Ä –∑–∞–∫–æ–Ω–∞ ¬´–û —Ä–µ–∫–ª–∞–º–µ¬ª —Å –ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–ü–ª—é—Å.
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ drafts/parse.py –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ PostgreSQL.
"""
import re
import time
import urllib.parse
from typing import List, Tuple, Dict
from datetime import date, datetime

import requests
from bs4 import BeautifulSoup, Tag
from sqlalchemy.orm import Session

from ..db import SessionLocal, engine
from ..models import LawVersion, LawArticle, LawChapter


# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
LAW_BASE_URL = "https://www.consultant.ru/document/cons_doc_LAW_58968/"
LAW_NAME = "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω –æ—Ç 13.03.2006 N 38-–§–ó ¬´–û —Ä–µ–∫–ª–∞–º–µ¬ª"
LAW_CODE = "38-FZ"

SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (compatible; law-scraper/1.0; +https://example.invalid)",
    "Accept-Language": "ru-RU,ru;q=0.9"
})


# -------------------- NETWORK -------------------- #
def fetch(url: str, *, retries: int = 3, sleep: float = 1.0) -> str:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    last_exc = None
    for i in range(retries):
        try:
            resp = SESSION.get(url, timeout=20)
            resp.raise_for_status()
            return resp.text
        except Exception as e:
            last_exc = e
            time.sleep(sleep * (i + 1))
    raise last_exc


# -------------------- HELPERS -------------------- #
def absolute(href: str, base: str) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π —Å—Å—ã–ª–∫–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—É—é"""
    return urllib.parse.urljoin(base, href)


# -------------------- TOC PARSING -------------------- #
def extract_structured_links(html: str, toc_url: str) -> List[Dict]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: –≥–ª–∞–≤—ã –∏ –∏—Ö —Å—Ç–∞—Ç—å–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: {"type": "chapter", "title": "...", "url": "...", "articles": [...]}
    """
    soup = BeautifulSoup(html, "lxml")
    allowed_prefix = "/document/cons_doc_LAW_58968/"
    structure = []
    
    # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –ø–æ–¥—Ä—è–¥, —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    all_links = soup.find_all("a", href=True)
    current_chapter = None
    seen_urls = set()  # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    
    for link in all_links:
        href = link["href"].strip()
        if not href.startswith(allowed_prefix):
            continue
        
        title = " ".join(link.get_text(" ", strip=True).split())
        if not title:
            continue
            
        url = absolute(href, toc_url)
        if "#" in url:
            url = url.split("#", 1)[0]
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π URL –∏ –¥—É–±–ª–∏–∫–∞—Ç—ã
        if url.rstrip("/") == toc_url.rstrip("/") or url in seen_urls:
            continue
        
        seen_urls.add(url)
        
        # –≠—Ç–æ –≥–ª–∞–≤–∞?
        if title.startswith("–ì–ª–∞–≤–∞"):
            current_chapter = {
                "type": "chapter",
                "title": title,
                "url": url,
                "articles": []
            }
            structure.append(current_chapter)
        
        # –≠—Ç–æ —Å—Ç–∞—Ç—å—è?
        elif title.startswith("–°—Ç–∞—Ç—å—è") and current_chapter:
            current_chapter["articles"].append({
                "title": title,
                "url": url
            })
    
    return structure


# -------------------- CONTENT PARSING -------------------- #
def clean_node_text(node: Tag) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    # –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞
    for sel in [
        ".info-link", ".document__insert", ".document__edit",
        ".dnk-button-dummy", ".document-page__balloon",
        ".full-text", ".document-page__banner-middle",
    ]:
        for bad in node.select(sel):
            bad.decompose()
    
    for bad in node.find_all(["script", "style", "noscript"]):
        bad.decompose()

    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    text = node.get_text("\n", strip=True)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
    lines = []
    
    i = 0
    text_lines = [l.strip() for l in text.split("\n") if l.strip()]
    
    while i < len(text_lines):
        line = text_lines[i]
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—É–Ω–∫—Ç—ã (1., 2., 3., 3.1. –∏ —Ç.–¥.)
        if re.match(r'^\d+(\.\d+)?\.', line):
            # –°–∫–ª–µ–∏–≤–∞–µ–º —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—É–Ω–∫—Ç–∞
            full_line = line
            i += 1
            while i < len(text_lines) and not re.match(r'^(\d+(\.\d+)?\.|[–∞-—è]\)|\d+\))', text_lines[i]):
                full_line += " " + text_lines[i]
                i += 1
            lines.append(f"\n\n{full_line}")
            continue
        
        # –ü–æ–¥–ø—É–Ω–∫—Ç—ã —Å –Ω–æ–º–µ—Ä–∞–º–∏ (1), 2), 3)) - –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        elif re.match(r'^\d+\)$', line):  # –¢–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
            # –°–∫–ª–µ–∏–≤–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            i += 1
            if i < len(text_lines):
                definition = text_lines[i]
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–∫–ª–µ–∏–≤–∞—Ç—å –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –Ω–æ–º–µ—Ä–∞
                i += 1
                while i < len(text_lines) and not re.match(r'^(\d+\)|[–∞-—è]\)|\d+(\.\d+)?\.)', text_lines[i]):
                    definition += " " + text_lines[i]
                    i += 1
                lines.append(f"\n{line} {definition}")
            else:
                lines.append(f"\n{line}")
            continue
        
        # –ü–æ–¥–ø—É–Ω–∫—Ç—ã —Å –±—É–∫–≤–∞–º–∏ (–∞), –±), –≤))
        elif re.match(r'^[–∞-—è]\)$', line, re.IGNORECASE):
            i += 1
            if i < len(text_lines):
                definition = text_lines[i]
                i += 1
                while i < len(text_lines) and not re.match(r'^([–∞-—è]\)|\d+\)|\d+(\.\d+)?\.)', text_lines[i]):
                    definition += " " + text_lines[i]
                    i += 1
                lines.append(f"\n  {line} {definition}")
            else:
                lines.append(f"\n  {line}")
            continue
        
        # –û–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        if lines:
            lines[-1] += " " + line
        else:
            lines.append(line)
        i += 1
    
    text = "\n".join(lines)
    
    # –û—á–∏—Å—Ç–∫–∞
    text = re.sub(r" +", " ", text)  # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"\n{4,}", "\n\n", text)  # –ú–∞–∫—Å–∏–º—É–º 2 –ø–µ—Ä–µ–Ω–æ—Å–∞ –ø–æ–¥—Ä—è–¥
    
    return text.strip()


def extract_clean_html(node: Tag) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ HTML —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    # –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    for sel in [
        ".info-link", ".document__insert", ".document__edit",
        ".dnk-button-dummy", ".document-page__balloon",
        ".full-text", ".document-page__banner-middle",
        "h1", "h2", "h3"  # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (–æ–Ω–∏ –æ—Ç–¥–µ–ª—å–Ω–æ)
    ]:
        for bad in node.select(sel):
            bad.decompose()
    
    for bad in node.find_all(["script", "style", "noscript"]):
        bad.decompose()
    
    # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å —Ç–µ–∫—Å—Ç–æ–º "–°—Ç–∞—Ç—å—è N..." –∏ —É–¥–∞–ª—è–µ–º –µ–≥–æ
    first_p = node.find("p")
    if first_p:
        text = first_p.get_text(strip=True)
        if re.match(r'^–°—Ç–∞—Ç—å—è\s+\d+', text):
            first_p.decompose()
    
    # –ü–æ–ª—É—á–∞–µ–º HTML
    html = str(node)
    
    # –û—á–∏—Å—Ç–∫–∞ –ª–∏—à–Ω–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
    html = re.sub(r'class="[^"]*"', '', html)
    html = re.sub(r'style="[^"]*"', '', html)
    html = re.sub(r'id="[^"]*"', '', html)
    html = re.sub(r'data-[a-z-]+="[^"]*"', '', html)
    
    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–≥–∏
    html = re.sub(r'<p>\s*</p>', '', html)
    html = re.sub(r'<div>\s*</div>', '', html)
    
    return html.strip()


def parse_article_page(html: str, url: str) -> Dict[str, str]:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏/–≥–ª–∞–≤—ã"""
    soup = BeautifulSoup(html, "lxml")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    title = ""
    title_el = soup.select_one(".document-page__content .doc-style h1")
    if title_el:
        title = " ".join(title_el.get_text(" ", strip=True).split())
    if not title:
        bc = soup.select_one(".document-page__breadcrumbs li:last-child")
        if bc:
            title = bc.get_text(" ", strip=True)
    if not title:
        title = soup.title.get_text(" ", strip=True) if soup.title else url

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_root = soup.select_one(".document-page__content")
    if not content_root:
        content_root = soup.select_one("section.document-page__main") or soup

    # Plain text –¥–ª—è –ø–æ–∏—Å–∫–∞
    body_text = clean_node_text(content_root)
    
    # HTML –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    body_html = extract_clean_html(content_root)
    
    return {"title": title, "content": body_text, "content_html": body_html, "url": url}


# -------------------- DATABASE OPERATIONS -------------------- #
def save_to_database(structure: List[Dict], version_date: date, law_name: str = LAW_NAME) -> None:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î —Å–æ —Å–≤—è–∑—è–º–∏ –≥–ª–∞–≤–∞-—Å—Ç–∞—Ç—å–∏"""
    db = SessionLocal()
    
    try:
        # 1. –î–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏
        db.query(LawVersion).filter_by(law_code=LAW_CODE, is_active=True).update({"is_active": False})
        db.commit()
        
        # 2. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
        law_version = LawVersion(
            law_name=law_name,
            law_code=LAW_CODE,
            source_url=LAW_BASE_URL,
            version_date=version_date,
            is_active=True
        )
        db.add(law_version)
        db.commit()
        db.refresh(law_version)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –≤–µ—Ä—Å–∏—è –∑–∞–∫–æ–Ω–∞ ID={law_version.id}, –¥–∞—Ç–∞={version_date}")
        
        # 3. –ü–∞—Ä—Å–∏–Ω–≥ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ: –≥–ª–∞–≤–∞ ‚Üí —Å—Ç–∞—Ç—å–∏
        total_count = 0
        for chapter_data in structure:
            # –ü–∞—Ä—Å–∏–º –≥–ª–∞–≤—É
            print(f"[–ì–ª–∞–≤–∞] –ü–∞—Ä—Å—é: {chapter_data['title']}")
            
            try:
                html = fetch(chapter_data["url"])
                parsed = parse_article_page(html, chapter_data["url"])
                
                match = re.search(r"–ì–ª–∞–≤–∞\s+(\d+)", chapter_data["title"])
                chapter_num = int(match.group(1)) if match else 0
                
                chapter = LawChapter(
                    version_id=law_version.id,
                    chapter_number=chapter_num,
                    title=parsed["title"],
                    content=parsed["content"],
                    source_url=parsed["url"]
                )
                db.add(chapter)
                db.commit()
                db.refresh(chapter)
                total_count += 1
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã: {e}")
                continue
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å–∏ —ç—Ç–æ–π –≥–ª–∞–≤—ã
            for article_data in chapter_data["articles"]:
                print(f"  [–°—Ç–∞—Ç—å—è] –ü–∞—Ä—Å—é: {article_data['title']}")
                
                try:
                    html = fetch(article_data["url"])
                    parsed = parse_article_page(html, article_data["url"])
                    
                    match = re.search(r"–°—Ç–∞—Ç—å—è\s+(\d+(?:\.\d+)?)", article_data["title"])
                    article_num = match.group(1) if match else "0"
                    
                    article = LawArticle(
                        version_id=law_version.id,
                        chapter_id=chapter.id,  # –°–≤—è–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é —Å –≥–ª–∞–≤–æ–π
                        article_number=article_num,
                        title=parsed["title"],
                        content=parsed["content"],
                        content_html=parsed.get("content_html"),  # HTML —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
                        source_url=parsed["url"]
                    )
                    db.add(article)
                    total_count += 1
                    
                    # –ö–æ–º–º–∏—Ç–∏–º –∫–∞–∂–¥—ã–µ 5 –∑–∞–ø–∏—Å–µ–π
                    if total_count % 5 == 0:
                        db.commit()
                    
                    time.sleep(0.7)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
                except Exception as e:
                    print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue
        
        db.commit()
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î")
        
    except Exception as e:
        db.rollback()
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
        raise
    finally:
        db.close()


def extract_law_metadata(html: str) -> Dict:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–æ–Ω–∞: –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –¥–∞—Ç–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"""
    soup = BeautifulSoup(html, "lxml")
    
    # –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–∫–æ–Ω–∞
    title_el = soup.select_one(".document-page__title h1")
    law_name = LAW_NAME  # default
    law_date = date(2006, 3, 13)  # default
    
    if title_el:
        title_text = title_el.get_text(strip=True)
        # –£–±–∏—Ä–∞–µ–º "(–ø–æ—Å–ª–µ–¥–Ω—è—è —Ä–µ–¥–∞–∫—Ü–∏—è)"
        law_name = re.sub(r'\s*\(–ø–æ—Å–ª–µ–¥–Ω—è—è —Ä–µ–¥–∞–∫—Ü–∏—è\)\s*$', '', title_text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è: "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω "–û —Ä–µ–∫–ª–∞–º–µ" –æ—Ç 13.03.2006 N 38-–§–ó"
        date_match = re.search(r'–æ—Ç\s+(\d{2}\.\d{2}\.\d{4})', law_name)
        if date_match:
            try:
                law_date = datetime.strptime(date_match.group(1), '%d.%m.%Y').date()
            except:
                pass
    
    return {
        "law_name": law_name,
        "version_date": law_date
    }


# -------------------- MAIN FUNCTION -------------------- #
def parse_and_save_law(law_url: str = LAW_BASE_URL) -> None:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–∫–æ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î.
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ manage.py –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ RQ.
    """
    print(f"üîç –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–∫–æ–Ω–∞: {law_url}")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è
    toc_html = fetch(law_url)
    
    # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = extract_law_metadata(toc_html)
    print(f"üìã –ó–∞–∫–æ–Ω: {metadata['law_name']}")
    print(f"üìÖ –î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–µ–¥–∞–∫—Ü–∏–∏: {metadata['version_date']}")
    
    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    structure = extract_structured_links(toc_html, law_url)
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    total_docs = len(structure) + sum(len(ch["articles"]) for ch in structure)
    print(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(structure)} –≥–ª–∞–≤, {total_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    save_to_database(structure, metadata["version_date"], metadata["law_name"])
    
    print("üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–∫–æ–Ω–∞ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")

